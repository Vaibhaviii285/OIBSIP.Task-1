# Fraud Detection Project 

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from imblearn.over_sampling import SMOTE

# 2. Load Dataset
# Download dataset from Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
data = pd.read_csv('creditcard.csv')
print(data.head())
print(data.info())
print(data['Class'].value_counts())  # Check number of fraud vs. normal

# 3. Exploratory Data Analysis (EDA)
plt.figure(figsize=(6,4))
sns.countplot(x='Class', data=data)
plt.title('Fraud vs Legitimate Transactions')
plt.show()

# Correlation heatmap
plt.figure(figsize=(12,8))
sns.heatmap(data.corr(), cmap='coolwarm', annot=False)
plt.title('Feature Correlations')
plt.show()

# 4. Data Preprocessing
# Separate features and target
X = data.drop('Class', axis=1)
y = data['Class']

# Standardize features (especially for PCA or distance-based models)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Handle Imbalanced Data
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_scaled, y)
print("After SMOTE, counts of label '1': {}".format(sum(y_res==1)))
print("After SMOTE, counts of label '0': {}".format(sum(y_res==0)))

# 6. Split Data
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

# 7. Model Training

# Logistic Regression
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

# 8. Model Evaluation
def evaluate_model(y_test, y_pred, model_name):
    print(f"--- {model_name} ---")
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()
    roc_auc = roc_auc_score(y_test, y_pred)
    print(f"ROC AUC Score: {roc_auc}\n")

evaluate_model(y_test, y_pred_lr, "Logistic Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest")
evaluate_model(y_test, y_pred_xgb, "XGBoost")

# 9. Feature Importance (for tree-based models)
feature_importances = pd.Series(rf.feature_importances_, index=data.columns[:-1])
feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(12,6))
plt.title('Random Forest Feature Importances')
plt.show()

# 10. Save the Best Model
import joblib
joblib.dump(rf, 'fraud_detection_model.pkl')
